{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe4e01-d4ce-4be2-9925-484e57b0fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pycocotools.mask import decode  # For decoding COCO RLE\n",
    "from skimage.transform import resize  # For resizing masks with nearest neighbor\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "os.chdir()\n",
    "# --- 1. Load Dataset Configuration from YAML ---\n",
    "yaml_config_path = \"sam2/configs/sam2.1_training/sam_train_val_json_win.yaml\"\n",
    "with open(yaml_config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "dataset_conf = config.get(\"dataset\", {})\n",
    "img_folder    = os.path.abspath(dataset_conf.get(\"img_folder\", \"\"))\n",
    "gt_folder     = os.path.abspath(dataset_conf.get(\"gt_folder\", \"\"))\n",
    "val_list_path = os.path.abspath(dataset_conf.get(\"val_list\", \"\"))\n",
    "\n",
    "print(\"Image folder:\", img_folder)\n",
    "print(\"Ground truth folder:\", gt_folder)\n",
    "print(\"Validation list:\", val_list_path)\n",
    "\n",
    "# --- 2. Read Validation List & Select a Sample ---\n",
    "with open(val_list_path, \"r\") as f:\n",
    "    val_files = [line.strip() for line in f if line.strip()]\n",
    "if not val_files:\n",
    "    raise ValueError(\"Validation list is empty!\")\n",
    "\n",
    "sample_filename = random.choice(val_files)\n",
    "print(\"Selected sample:\", sample_filename)\n",
    "\n",
    "def find_file(folder, base_name, extensions):\n",
    "    for ext in extensions:\n",
    "        candidate = os.path.join(folder, base_name + ext)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"No file found for {base_name} in {folder} with extensions {extensions}\")\n",
    "\n",
    "sample_img_path  = find_file(img_folder, sample_filename, [\".jpg\", \".jpeg\"])\n",
    "sample_mask_path = find_file(gt_folder, sample_filename, [\".json\"])\n",
    "print(\"Sample image path:\", sample_img_path)\n",
    "print(\"Sample mask path:\", sample_mask_path)\n",
    "\n",
    "# --- 3. Load Image & Decode COCO RLE Ground Truth Mask ---\n",
    "opened_image = np.array(Image.open(sample_img_path).convert(\"RGB\"))\n",
    "\n",
    "with open(sample_mask_path, \"r\") as f:\n",
    "    mask_data = json.load(f)\n",
    "print(\"Mask JSON keys:\", list(mask_data.keys()))\n",
    "# Expected keys: ['info', 'licenses', 'images', 'annotations']\n",
    "\n",
    "annotations = mask_data.get(\"annotations\", [])\n",
    "if not annotations:\n",
    "    raise ValueError(\"No annotations found in mask JSON!\")\n",
    "rle_data = annotations[0].get(\"segmentation\", None)\n",
    "if rle_data is None:\n",
    "    raise ValueError(\"No RLE segmentation found in the annotation!\")\n",
    "height, width = rle_data[\"size\"]\n",
    "rle_counts = rle_data[\"counts\"]\n",
    "ground_truth = decode({\"size\": [height, width], \"counts\": rle_counts})\n",
    "ground_truth = (ground_truth > 0).astype(np.uint8) * 255  # Binary mask: foreground = 255\n",
    "print(f\"Decoded ground truth mask shape: {ground_truth.shape}\")\n",
    "\n",
    "# --- 4. Resize Image and Ground Truth while Maintaining Aspect Ratio ---\n",
    "max_dimension = 1024  # Maximum dimension for width or height\n",
    "h, w = opened_image.shape[:2]\n",
    "scale = min(max_dimension / w, max_dimension / h)\n",
    "target_h, target_w = int(h * scale), int(w * scale)\n",
    "print(f\"Resizing scale: {scale:.3f}, target size: ({target_w}, {target_h})\")\n",
    "\n",
    "# Resize image using cv2 (linear interpolation)\n",
    "resized_image = cv2.resize(opened_image, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "print(\"Processed (Resized) image shape:\", resized_image.shape)\n",
    "\n",
    "# Resize ground truth mask using skimage's resize \n",
    "original_dtype = ground_truth.dtype\n",
    "mask_resized = resize(ground_truth, (target_h, target_w), order=0,\n",
    "                      preserve_range=True, anti_aliasing=False)\n",
    "mask_resized = np.round(mask_resized).astype(original_dtype)\n",
    "print(\"Resized ground truth mask shape:\", mask_resized.shape)\n",
    "\n",
    "# --- 5. Sample Foreground and Background Points for Predictor ---\n",
    "def get_points_for_inference(mask, num_points_fg, num_points_bg):\n",
    "    \"\"\"Samples foreground (mask > 0) and background (mask == 0) points.\"\"\"\n",
    "    points = []\n",
    "    labels = []\n",
    "    fg_coords = np.argwhere(mask > 0)\n",
    "    if len(fg_coords) > 0:\n",
    "        num_samples = min(len(fg_coords), num_points_fg)\n",
    "        sampled_indices = np.random.choice(len(fg_coords), num_samples, replace=False)\n",
    "        for idx in sampled_indices:\n",
    "            y, x = fg_coords[idx]\n",
    "            points.append([float(x), float(y)])  # (x, y)\n",
    "            labels.append(1)\n",
    "    bg_coords = np.argwhere(mask == 0)\n",
    "    if len(bg_coords) > 0:\n",
    "        num_samples = min(len(bg_coords), num_points_bg)\n",
    "        sampled_indices = np.random.choice(len(bg_coords), num_samples, replace=False)\n",
    "        for idx in sampled_indices:\n",
    "            y, x = bg_coords[idx]\n",
    "            points.append([float(x), float(y)])\n",
    "            labels.append(0)\n",
    "    if not points:\n",
    "        return None, None\n",
    "    return np.array(points, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "\n",
    "input_points, input_labels = get_points_for_inference(mask_resized, num_points_fg=10, num_points_bg=10)\n",
    "if input_points is None:\n",
    "    raise ValueError(\"No points sampled for inference!\")\n",
    "# Print counts\n",
    "num_fg_points = np.sum(input_labels == 1)\n",
    "num_bg_points = np.sum(input_labels == 0)\n",
    "\n",
    "print(f\"Foreground points sampled: {num_fg_points}\")\n",
    "print(f\"Background points sampled: {num_bg_points}\")\n",
    "\n",
    "# --- 6. Load SAM-2.1 Model & Run Point-Based Predictions ---\n",
    "model_cfg       = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "checkpoint_path = \"sam2_logs/epochs_1000_LR_0.01_prob_to_use_pt_input_for_train_1.0_prob_to_sample_from_gt_for_train_1.0/checkpoints/checkpoint.pt\"\n",
    "sam2_model = build_sam2(model_cfg, checkpoint_path, device=\"cuda\")\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# Perform inference using point prompts.\n",
    "with torch.no_grad():\n",
    "    predictor.set_image(resized_image)  # Set the resized image\n",
    "    masks_pred, scores_pred, logits_pred = predictor.predict(\n",
    "        point_coords=input_points,  # Sampled points\n",
    "        point_labels=input_labels,  # Labels (foreground/background)\n",
    "        multimask_output=True  # Get multiple masks per prompt if model supports\n",
    "    )\n",
    "\n",
    "# Select the best mask (highest score across prompts)\n",
    "best_mask_idx = np.argmax(scores_pred)\n",
    "pred_mask = masks_pred[best_mask_idx]  # Boolean mask\n",
    "pred_mask_vis = (pred_mask.astype(np.uint8)) * 255  # Convert to uint8 format\n",
    "\n",
    "# --- 7. Count Instances in Predicted Mask ---\n",
    "def count_connected_instances(mask):\n",
    "    \"\"\"Counts connected components in a binary mask, excluding background.\"\"\"\n",
    "    mask_uint8 = (mask > 0).astype(np.uint8) * 255\n",
    "    num_labels, _ = cv2.connectedComponents(mask_uint8)\n",
    "    return num_labels - 1 if num_labels > 0 else 0\n",
    "gt_instance_count = count_connected_instances(mask_resized)\n",
    "pred_instance_count = count_connected_instances(pred_mask)\n",
    "print(f\"Predicted instances: {pred_instance_count}\")\n",
    "\n",
    "# --- 8. Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(opened_image)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(mask_resized, cmap=\"gray\")\n",
    "axes[1].set_title(f\"Ground Truth Mask ({gt_instance_count} instances)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(pred_mask_vis, cmap=\"gray\")\n",
    "axes[2].set_title(f\"Predicted Mask ({pred_instance_count} instances)\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
